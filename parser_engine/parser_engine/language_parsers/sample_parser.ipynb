{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "# Add the project root to Python path\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "\n",
    "# Now your original imports should work\n",
    "from parser_engine.language_parsers.python_parser import PythonParser\n",
    "from parser_engine.models.data_models import *\n",
    "from parser_engine.core.repo_analyzer import RepoIndexer\n",
    "\n",
    "repo_path = Path('/Users/qudi/Desktop/workspace/third-party/fairscale/fairscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print module name\n",
    "def print_function(func: FunctionElement):\n",
    "    print(func.name)\n",
    "\n",
    "def print_class(cls: ClassElement):\n",
    "    print(cls.name)\n",
    "    for func in cls.methods:\n",
    "        print_function(func)\n",
    "\n",
    "def print_module(module: ModuleElement):\n",
    "    for cls in module.classes:\n",
    "        print_class(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairscale.version\n",
      "fairscale.__init__\n",
      "experimental.__init__\n",
      "tooling.layer_memory_tracker\n",
      "tooling.layer_memory_tracker.find_best_reset_points(activation_sizes: List[int], num_checkpoints: int) -> Tuple[int, List[int]]\n",
      "tooling.layer_memory_tracker.suggest_checkpoint_location(traces: List[LayerMemoryTrace], num_checkpoints: int, num_skipped_layers: int) -> SuggestedCheckpoints\n",
      "tooling.layer_memory_tracker._assert_visualisation_library_installed() -> None\n",
      "tooling.layer_memory_tracker.compare_memory_traces_in_plot(memory_traces_by_job: Dict[str, List[LayerMemoryTrace]], figsize: Tuple[int, int], capture: bool) -> Optional[Any]\n",
      "tooling.layer_memory_tracker.null_context() -> Iterator[None]\n",
      "tooling.layer_memory_tracker.matplotlib_figure_to_image(fig: Any) -> Any\n",
      "tooling.__init__\n",
      "nn.auto_shard\n",
      "nn.auto_shard._get_count(param_count: Dict, node_name: str) -> int\n",
      "nn.auto_shard._create_shard_to_param_count(param_count: Dict, node_name_to_shard_id: Dict) -> Dict\n",
      "nn.auto_shard._split_nodes(traced_graph_module: torch.fx.GraphModule, shard_count: int) -> Dict\n",
      "nn.auto_shard._trace(model: torch.nn.Module) -> torch.fx.GraphModule\n",
      "nn.auto_shard.shard_model(model: torch.nn.Module, shard_count: int) -> List[torch.fx.GraphModule]\n",
      "nn.__init__\n",
      "nn.sync_batchnorm\n",
      "nn.sync_batchnorm._forward(input: Tensor, affine: bool, mean: Tensor, invstd: Tensor, weight: Tensor, bias: Tensor) -> Tensor\n",
      "nn.sync_batchnorm._track_running_stats(running_mean: Tensor, running_var: Tensor, momentum: float, mean: Tensor, var: Tensor, total_count: Tensor) -> None\n",
      "nn.sync_batchnorm._calculate_stats(input: Tensor, eps: float, process_group: ProcessGroup) -> Tuple[Tensor, Tensor, Tensor, Tensor]\n",
      "nn.mevo\n",
      "nn.mevo._next_power_of_2_or_max(n: int, max_n: int) -> int\n",
      "nn.mevo._reshape_inputs(input: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]\n",
      "nn.mevo.get_data(shape: Tuple[Tuple[int, int], Tuple[int, int]], dtype: torch.dtype, device: str) -> Tuple[torch.Tensor, nn.Parameter, torch.Tensor]\n",
      "nn.mevo.lmcl_matmul(i: torch.Tensor, w: torch.Tensor, tgt: torch.Tensor, w_idx: int, margin: float, scale: Optional[float]) -> torch.Tensor\n",
      "nn.offload\n",
      "nn.offload._conditional_amp_fwd_decorator(orig_func: Any) -> Any\n",
      "nn.offload._conditional_amp_bwd_decorator(orig_func: Any) -> Any\n",
      "nn.offload._split(modules: nn.Sequential, number_splits: int) -> List[List[nn.Module]]\n",
      "ampnet_pipe.ampnet\n",
      "ampnet_pipe.ampnet.create_task_without_skip_trackers(checkpoint_stop: int, i: int, j: int, batch: Batch, partition: nn.Sequential) -> Task\n",
      "ampnet_pipe.__init__\n",
      "ampnet_pipe.pipe\n",
      "data_parallel.__init__\n",
      "gossip.mixing_manager\n",
      "gossip.__init__\n",
      "gossip.distributed\n",
      "gossip.graph_manager\n",
      "gossip.gossiper\n",
      "utils.__init__\n",
      "utils.cuda_metering\n",
      "utils.cuda_metering.create_and_record_event() -> torch.cuda.Event\n",
      "utils.cuda_metering.create_event_recorder(event_name: str, dummy: bool) -> EventRecorder\n",
      "utils.helpers\n",
      "utils.helpers.flatten_tensors(tensors: List[torch.Tensor]) -> torch.Tensor\n",
      "utils.helpers.unflatten_tensors(flat: torch.Tensor, tensors: List[torch.Tensor]) -> List[torch.Tensor]\n",
      "utils.helpers.group_by_dtype(tensors: List[torch.Tensor]) -> Dict[torch.dtype, List[torch.Tensor]]\n",
      "utils.helpers.communicate(tensors: List[torch.Tensor], communication_op: Any, logger: logging.Logger) -> None\n",
      "utils.helpers.make_logger(rank: int, verbose: bool) -> logging.Logger\n",
      "utils.helpers.create_process_group(ranks: List[int]) -> torch.distributed.ProcessGroup\n",
      "distributed_pipeline.trace\n",
      "distributed_pipeline.trace._call_trace(tracer: RemoteModuleTracer, module: nn.Module) -> torch.fx.Graph\n",
      "distributed_pipeline.trace.make_graph(module: nn.Module) -> PipelineModulesGraph\n",
      "distributed_pipeline.graph\n",
      "distributed_pipeline.graph.RemoteSequential(rref_list: List[rpc.RRef]) -> MultiInputSequential\n",
      "distributed_pipeline.__init__\n",
      "distributed_pipeline.partition_handler\n",
      "distributed_pipeline.loss\n",
      "distributed_pipeline.loss._rloss(loss_func: Callable, input_rref: rpc.RRef, target_rref: rpc.RRef) -> rpc.RRef\n",
      "distributed_pipeline.loss.DistributedLoss(loss: nn.Module) -> Callable\n",
      "distributed_pipeline.pipeline\n",
      "distributed_pipeline.pipeline.check_pytorch_version() -> None\n",
      "distributed_pipeline.data\n",
      "optim.dynamic_loss_scaler\n",
      "optim.dynamic_loss_scaler._refresh_per_optimizer_state() -> OptState\n",
      "optim.__init__\n",
      "wgit.signal_sparsity\n",
      "wgit.signal_sparsity._get_k_for_topk(topk_percent: Optional[float], top_k_element: Optional[int], top_k_total_size: int) -> int\n",
      "wgit.signal_sparsity._scatter_topk_to_sparse_tensor(top_k_tensor: Tensor, to_be_sparsify_tensor: Tensor, k: int, dim: Optional[int]) -> Tensor\n",
      "wgit.signal_sparsity._top_k_total_size(tensor: Tensor, topk_dim: Optional[int]) -> int\n",
      "wgit.signal_sparsity._is_sparsity_zero(dense: Tensor, topk_percent: Optional[float], topk_element: Optional[int], top_k_dim: Optional[int]) -> bool\n",
      "wgit.signal_sparsity._fft_transform(dense: Tensor, dim: int) -> Tensor\n",
      "wgit.signal_sparsity._ifft_transform(sst: Tensor, dim: int) -> Tensor\n",
      "wgit.signal_sparsity._dct_transform(dense: Tensor, dim: int) -> Tensor\n",
      "wgit.signal_sparsity._idct_transform(sst: Tensor, dim: int) -> Tensor\n",
      "wgit.signal_sparsity.random_sparse_mask(dense: Tensor, percent: float, dim: int) -> Tensor\n",
      "wgit.version\n",
      "wgit.signal_sparsity_profiling\n",
      "wgit.__init__\n",
      "wgit.cli\n",
      "wgit.cli.main(argv: List[str]) -> None\n",
      "wgit.utils\n",
      "wgit.repo\n",
      "wgit.repo._recursive_apply_to_elements(data: Union[List[Any], Dict[str, Any]], fn: Any, names: List[str]) -> None\n",
      "wgit.pygit\n",
      "wgit.__main__\n",
      "wgit.sha1_store\n",
      "wgit.sha1_store._get_json_entry(d: Dict[str, Any]) -> Dict[str, Any]\n",
      "wgit.sha1_store._copy_compressed(src: Path, dest: Path, thread: Optional[int], blocksize: int) -> Tuple[int, int]\n",
      "wgit.sha1_store._copy_uncompressed(src: Path, dest: Path, thread: Optional[int], blocksize: int) -> None\n",
      "fair_dev.__init__\n",
      "fair_dev.common_paths\n",
      "testing.testing_memory\n",
      "testing.testing_memory.find_tensor_by_shape(target_shape: Tuple, only_param: bool) -> bool\n",
      "testing.__init__\n",
      "testing.testing\n",
      "testing.testing.set_random_seed(seed: int, model_parallel: bool) -> None\n",
      "testing.testing.in_circle_ci() -> bool\n",
      "testing.testing.torch_cuda_version(compiled: bool) -> Tuple[int, ...]\n",
      "testing.testing.make_cudnn_deterministic() -> None\n",
      "testing.testing.dist_init(rank: int, world_size: int, filename: str, filename_rpc: str) -> bool\n",
      "testing.testing.get_worker_map() -> Dict[Any, Any]\n",
      "testing.testing.get_world_sizes() -> List[int]\n",
      "testing.testing.test_runner(rank: int, test_func: Callable, deterministic: bool) -> None\n",
      "testing.testing.spawn_for_all_world_sizes(test_func: Callable, world_sizes: List[int], args: Any, deterministic: bool) -> None\n",
      "testing.testing.worker_process(rank: int, world_size: int, filename: str, filename_rpc: str, func: Callable, args: Any, error_queue: Any) -> None\n",
      "testing.testing.teardown() -> None\n",
      "testing.testing.torch_spawn(world_sizes: Optional[List[int]]) -> Callable\n",
      "testing.testing.objects_are_equal(a: Any, b: Any, raise_exception: bool, dict_key: Optional[str], rtol: Optional[float], atol: Optional[float]) -> bool\n",
      "testing.testing.check_same_model_params(model_a: torch.nn.Module, model_b: torch.nn.Module, message: str) -> None\n",
      "testing.testing.check_same_models_across_ranks(model: torch.nn.Module, process_group: Any, params_should_be_equal: bool, check_broadcast_buffers: bool) -> None\n",
      "testing.testing.get_cycles_per_ms() -> float\n",
      "testing.testing.state_dict_norm(state: Dict[str, torch.Tensor]) -> torch.Tensor\n",
      "testing.testing.rmf(filename: str) -> None\n",
      "testing.testing.in_temporary_directory() -> Generator\n",
      "testing.testing.temp_files_ctx(num: int) -> Generator\n",
      "testing.testing.dump_all_tensors(rank: int) -> None\n",
      "testing.testing.get_smi_memory() -> float\n",
      "testing.testing.skip_a_test_if_in_CI() -> None\n",
      "testing.golden_testing_data\n",
      "nn.__init__\n",
      "misc.flatten_params_wrapper\n",
      "misc.flatten_params_wrapper._post_state_dict_hook(module: nn.Module, state_dict: 'OrderedDict[str, Tensor]', prefix: str) -> 'OrderedDict[str, Tensor]'\n",
      "misc.flatten_params_wrapper._pre_load_state_dict_hook(state_dict: Union[Dict[str, Tensor], 'OrderedDict[str, Tensor]'], prefix: str) -> None\n",
      "misc.__init__\n",
      "misc.param_bucket\n",
      "data_parallel.fully_sharded_data_parallel\n",
      "data_parallel.fully_sharded_data_parallel.p_assert(cond: Any, s: Any) -> None\n",
      "data_parallel.fully_sharded_data_parallel._get_default_cuda_device(module: nn.Module) -> torch.device\n",
      "data_parallel.fully_sharded_data_parallel.cast_floats_to_right_precision(to_fp16: bool, no_grad: bool) -> Tuple[Any, Any]\n",
      "data_parallel.fully_sharded_data_parallel.free_storage_(data: torch.Tensor) -> None\n",
      "data_parallel.fully_sharded_data_parallel.alloc_storage_(data: torch.Tensor, size: torch.Size) -> None\n",
      "data_parallel.fully_sharded_data_parallel._post_state_dict_hook(state_dict_on_rank_0_only: bool, module: FullyShardedDataParallel, state_dict: 'OrderedDict[str, torch.Tensor]', prefix: str) -> 'OrderedDict[str, torch.Tensor]'\n",
      "data_parallel.fully_sharded_data_parallel.no_pre_load_state_dict_hook() -> Generator\n",
      "data_parallel.fully_sharded_data_parallel._pre_load_state_dict_hook(state_dict: Union[Dict[str, torch.Tensor], 'OrderedDict[str, torch.Tensor]'], prefix: str) -> None\n",
      "data_parallel.fully_sharded_data_parallel._clean_path(path: str) -> str\n",
      "data_parallel.fully_sharded_data_parallel._unpad(shard: torch.Tensor, pad: int) -> torch.Tensor\n",
      "data_parallel.fully_sharded_data_parallel.auto_wrap_bn(module: nn.Module, single_rank_pg: bool, process_group: Optional['ProcessGroup'], fsdp_config: Optional[Dict[str, Any]], wrap_it: bool, assert_on_collision: bool) -> nn.Module\n",
      "data_parallel.fully_sharded_data_parallel.get_fsdp_instances(mod: nn.Module, skip_empty: bool) -> List[FullyShardedDataParallel]\n",
      "data_parallel.__init__\n",
      "data_parallel.fsdp_optim_utils\n",
      "data_parallel.fsdp_optim_utils.flatten_optim_state_dict(sd: Dict) -> Dict\n",
      "data_parallel.fsdp_optim_utils.check_param_counts_before_sharding(full_optim_state_dict: Dict, n_instances: int) -> None\n",
      "data_parallel.fsdp_optim_utils._extract_non_tensor_state(combined_state: Dict[int, Dict[str, List]], param_id: int) -> Dict\n",
      "data_parallel.fsdp_optim_utils._unflatten_optim_state(combined_state: Dict[int, Dict], instance_list: List['FullyShardedDataParallel'], world_pad_info: List[List[List[int]]], singleton_state: Dict[int, Dict]) -> Tuple[Dict[int, Dict], Dict[int, int]]\n",
      "data_parallel.fsdp_optim_utils.build_unflat_state_dict(instance_list: List['FullyShardedDataParallel'], world_pad_info: List[List[List[int]]], state: Dict[int, Dict[str, List[torch.Tensor]]], singleton_state: Dict[int, Dict[str, List[torch.Tensor]]], uncollected_opt_state: Dict[int, Dict], original_sd: Dict) -> Dict\n",
      "data_parallel.fsdp_optim_utils.is_singleton_tensor(x: Any) -> bool\n",
      "data_parallel.sharded_ddp\n",
      "data_parallel.sharded_ddp._trainable(param: torch.Tensor) -> bool\n",
      "pipe.worker\n",
      "pipe.worker.worker(in_queue: InQueue, out_queue: OutQueue, device: torch.device) -> None\n",
      "pipe.worker.create_workers(devices: List[torch.device]) -> Tuple[List[InQueue], List[OutQueue]]\n",
      "pipe.worker.join_workers(in_queues: List[InQueue], out_queues: List[OutQueue]) -> None\n",
      "pipe.worker.spawn_workers(devices: List[torch.device]) -> Generator[Tuple[List[InQueue], List[OutQueue]], None, None]\n",
      "pipe.phony\n",
      "pipe.phony.get_phony(device: torch.device) -> Tensor\n",
      "pipe.checkpoint\n",
      "pipe.checkpoint.enable_checkpointing() -> Generator[None, None, None]\n",
      "pipe.checkpoint.enable_recomputing() -> Generator[None, None, None]\n",
      "pipe.checkpoint.is_checkpointing() -> bool\n",
      "pipe.checkpoint.is_recomputing() -> bool\n",
      "pipe.checkpoint.save_rng_states(device: torch.device, rng_states: Deque[RNGStates]) -> None\n",
      "pipe.checkpoint.restore_rng_states(device: torch.device, rng_states: Deque[RNGStates]) -> Generator[None, None, None]\n",
      "pipe.batchnorm\n",
      "pipe.__init__\n",
      "pipe.copy\n",
      "pipe.types\n",
      "pipe.stream\n",
      "pipe.stream.new_stream(device: torch.device) -> AbstractStream\n",
      "pipe.stream.current_stream(device: torch.device) -> AbstractStream\n",
      "pipe.stream.default_stream(device: torch.device) -> AbstractStream\n",
      "pipe.stream.use_device(device: torch.device) -> Generator[None, None, None]\n",
      "pipe.stream.use_stream(stream: Optional[AbstractStream]) -> Generator[None, None, None]\n",
      "pipe.stream.get_device(stream: AbstractStream) -> torch.device\n",
      "pipe.stream.wait_stream(source: AbstractStream, target: AbstractStream) -> None\n",
      "pipe.stream.record_stream(tensor: torch.Tensor, stream: AbstractStream) -> None\n",
      "pipe.stream.is_cuda(stream: Optional[AbstractStream]) -> bool\n",
      "pipe.stream.as_cuda(stream: AbstractStream) -> torch.cuda.Stream\n",
      "pipe.rpc\n",
      "pipe.rpc.set_device_based_on_group(group: ProcessGroup) -> None\n",
      "pipe.rpc.get_shapes(tensor: TensorOrTensors) -> SizeOrSizes\n",
      "pipe.rpc.get_dtype(tensor: TensorOrTensors) -> DtypeOrDtypes\n",
      "pipe.rpc.get_global_ranks_from_group(group: ProcessGroup) -> List[int]\n",
      "pipe.rpc.callback_with_model(callback: Callable[[Any, AsyncPipe], None], ctx: Any) -> None\n",
      "pipe.async_schedule\n",
      "pipe.async_schedule.create_task(checkpoint_stop: int, chunk_id: int, part_id: int, batch: Batch, partition: nn.Sequential, skip_trackers: List[SkipTrackerThroughPotals]) -> Task\n",
      "pipe.pipeline\n",
      "pipe.pipeline.depend(fork_from: Batch, join_to: Batch) -> None\n",
      "pipe.pipeline.copy(batch: Batch, prev_stream: AbstractStream, next_stream: AbstractStream) -> None\n",
      "pipe.pipeline.wait(batch: Batch, prev_stream: AbstractStream, next_stream: AbstractStream) -> None\n",
      "pipe.pipeline.clock_cycles(m: int, n: int) -> Iterable[List[Tuple[int, int]]]\n",
      "pipe.microbatch\n",
      "pipe.microbatch.check(input: TensorOrTensors) -> None\n",
      "pipe.microbatch.scatter(input: TensorOrTensors, chunks: int) -> List[Batch]\n",
      "pipe.microbatch.gather(outputs: List[Batch]) -> TensorOrTensors\n",
      "pipe.dependency\n",
      "pipe.dependency.fork(input: Tensor) -> Tuple[Tensor, Tensor]\n",
      "pipe.dependency.join(input: Tensor, phony: Tensor) -> Tensor\n",
      "pipe.messages\n",
      "pipe.messages.to_input_device(tensors: Tensors, input_device: InputDevice) -> Tensors\n",
      "pipe.messages.rpc_push_queue(message: PipeMessage) -> None\n",
      "pipe.messages.MakeTransport(use_rpc: bool, worker_map: Optional[Dict[int, str]], input_device: InputDevice) -> Transport\n",
      "pipe.async_pipe\n",
      "pipe.async_pipe.verify_module(module: Union[nn.Sequential, List[LazyModule]]) -> None\n",
      "pipe.async_pipe.check_balance(module: Union[nn.Sequential, List[LazyModule]], balance: List[int]) -> None\n",
      "pipe.pipe\n",
      "pipe.pipe.recommend_auto_balance(message: str) -> str\n",
      "pipe.pipe.verify_module(module: nn.Sequential) -> None\n",
      "pipe.pipe.verify_splitting(module: nn.Sequential, partitions: List[nn.Sequential], balance: Iterable[int], devices: List[torch.device]) -> None\n",
      "pipe.pipe.split_module(module: nn.Sequential, balance: Iterable[int], devices: List[torch.device]) -> Tuple[List[nn.Sequential], List[int], List[torch.device]]\n",
      "pipe.async_pipeline\n",
      "balance.profile\n",
      "balance.profile.layerwise_sandbox(module: nn.Sequential, device: torch.device) -> Generator[nn.Module, None, None]\n",
      "balance.profile.detach(batch: Batch) -> None\n",
      "balance.profile.profile_times(module: nn.Sequential, sample: TensorOrTensors, timeout: float, device: torch.device) -> List[int]\n",
      "balance.profile.profile_sizes(module: nn.Sequential, input: TensorOrTensors, chunks: int, param_scale: float, device: torch.device) -> List[int]\n",
      "balance.__init__\n",
      "balance.__init__._balance_cost(cost: List[int], partitions: int) -> List[int]\n",
      "balance.__init__.balance_by_time(partitions: int, module: nn.Sequential, sample: TensorOrTensors) -> List[int]\n",
      "balance.__init__.balance_by_size(partitions: int, module: nn.Sequential, input: TensorOrTensors) -> List[int]\n",
      "balance.blockpartition\n",
      "balance.blockpartition.solve(sequence: List[int], partitions: int) -> List[List[int]]\n",
      "skip.portal\n",
      "skip.layout\n",
      "skip.layout.inspect_skip_layout(partitions: List[nn.Sequential]) -> SkipLayout\n",
      "skip.__init__\n",
      "skip.tracker\n",
      "skip.tracker.use_skip_tracker(skip_tracker: SkipTracker) -> Generator[None, None, None]\n",
      "skip.tracker.current_skip_tracker() -> SkipTracker\n",
      "skip.namespace\n",
      "skip.skippable\n",
      "skip.skippable.skippable(stash: Iterable[str], pop: Iterable[str]) -> Callable[[Type[SkippableModule]], Type[Skippable]]\n",
      "skip.skippable.verify_skippables(module: nn.Sequential) -> None\n",
      "checkpoint.checkpoint_activations\n",
      "checkpoint.checkpoint_activations.disable_checkpointing() -> Generator[None, None, None]\n",
      "checkpoint.checkpoint_activations.enable_checkpointing() -> Generator[None, None, None]\n",
      "checkpoint.checkpoint_activations.enable_recomputing() -> Generator[None, None, None]\n",
      "checkpoint.checkpoint_activations.is_checkpointing() -> bool\n",
      "checkpoint.checkpoint_activations.is_recomputing() -> bool\n",
      "checkpoint.checkpoint_activations.checkpoint_wrapper(module: nn.Module, offload_to_cpu: bool) -> nn.Module\n",
      "checkpoint.checkpoint_activations._checkpointed_forward(original_forward: Any, weak_self: Any, offload_to_cpu: bool) -> Any\n",
      "checkpoint.checkpoint_activations.get_rng_state() -> Dict[str, Any]\n",
      "checkpoint.checkpoint_activations.set_rng_state(state: Dict[str, Any]) -> None\n",
      "checkpoint.checkpoint_activations.is_autocast_enabled() -> bool\n",
      "checkpoint.checkpoint_activations.autocast(enabled: bool) -> Generator\n",
      "checkpoint.__init__\n",
      "checkpoint.checkpoint_utils\n",
      "checkpoint.checkpoint_utils.patch_batchnorm(module: nn.Module) -> List\n",
      "model_parallel.cross_entropy\n",
      "model_parallel.cross_entropy.vocab_parallel_cross_entropy(vocab_parallel_logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor\n",
      "model_parallel.initialize\n",
      "model_parallel.initialize.initialize_model_parallel(model_parallel_size_: int, pipeline_length: int, context_parallel_size: int) -> None\n",
      "model_parallel.initialize.model_parallel_is_initialized() -> bool\n",
      "model_parallel.initialize.get_context_parallel_group() -> torch.distributed.ProcessGroup\n",
      "model_parallel.initialize.get_context_parallel_ranks() -> List[int]\n",
      "model_parallel.initialize.get_context_parallel_world_size() -> int\n",
      "model_parallel.initialize.get_context_parallel_rank() -> int\n",
      "model_parallel.initialize.get_model_parallel_group() -> torch.distributed.ProcessGroup\n",
      "model_parallel.initialize.get_data_parallel_group() -> torch.distributed.ProcessGroup\n",
      "model_parallel.initialize.get_pipeline_parallel_group() -> torch.distributed.ProcessGroup\n",
      "model_parallel.initialize.get_pipeline_parallel_ranks() -> List[int]\n",
      "model_parallel.initialize.get_model_parallel_world_size() -> int\n",
      "model_parallel.initialize.get_model_parallel_rank() -> int\n",
      "model_parallel.initialize.get_model_parallel_src_rank() -> int\n",
      "model_parallel.initialize.get_data_parallel_world_size() -> int\n",
      "model_parallel.initialize.get_data_parallel_rank() -> int\n",
      "model_parallel.initialize.destroy_model_parallel() -> None\n",
      "model_parallel.__init__\n",
      "model_parallel.random\n",
      "model_parallel.random._set_cuda_rng_state(new_state: torch.ByteTensor, device: Union[int, str, torch.device]) -> None\n",
      "model_parallel.random.get_cuda_rng_tracker() -> CudaRNGStatesTracker\n",
      "model_parallel.random.model_parallel_cuda_manual_seed(seed: int) -> None\n",
      "model_parallel.random.checkpoint(function: Any) -> Any\n",
      "model_parallel.utils\n",
      "model_parallel.utils.ensure_divisibility(numerator: int, denominator: int) -> None\n",
      "model_parallel.utils.divide_and_check_no_remainder(numerator: int, denominator: int) -> int\n",
      "model_parallel.utils.split_tensor_along_last_dim(tensor: torch.Tensor, num_partitions: int, contiguous_split_chunks: bool) -> Tuple[torch.Tensor, ...]\n",
      "model_parallel.layers\n",
      "model_parallel.layers._initialize_affine_weight(weight: torch.Tensor, out_features: int, in_features: int, per_partition_size: int, partition_dim: int, init_method: Callable[[torch.Tensor], torch.Tensor], stride: int, return_master_weight: bool) -> Optional[torch.Tensor]\n",
      "model_parallel.mappings\n",
      "model_parallel.mappings._reduce(ctx: Any, input_: torch.Tensor) -> torch.Tensor\n",
      "model_parallel.mappings._split(input_: torch.Tensor) -> torch.Tensor\n",
      "model_parallel.mappings._gather(input_: torch.Tensor) -> torch.Tensor\n",
      "model_parallel.mappings.copy_to_model_parallel_region(input_: torch.Tensor) -> torch.Tensor\n",
      "model_parallel.mappings.reduce_from_model_parallel_region(input_: torch.Tensor) -> torch.Tensor\n",
      "model_parallel.mappings.scatter_to_model_parallel_region(input_: torch.Tensor) -> torch.Tensor\n",
      "model_parallel.mappings.gather_from_model_parallel_region(input_: torch.Tensor) -> torch.Tensor\n",
      "wrap.auto_wrap\n",
      "wrap.auto_wrap.default_auto_wrap_policy(module: nn.Module, recurse: bool, unwrapped_params: int, module_is_root: bool, min_num_params: int, force_leaf_modules: Optional[Set[Type[nn.Module]]], exclude_wrap_modules: Optional[Set[Type[nn.Module]]], skip_params_check_for_root: bool) -> bool\n",
      "wrap.auto_wrap.config_auto_wrap_policy(module: nn.Module, recurse: bool, unwrapped_params: int, module_is_root: bool) -> bool\n",
      "wrap.auto_wrap.enable_wrap(auto_wrap_policy: Optional[Callable]) -> Generator[None, None, None]\n",
      "wrap.auto_wrap.wrap(module: nn.Module) -> nn.Module\n",
      "wrap.auto_wrap.auto_wrap(module: nn.Module, auto_wrap_policy: Optional[Callable]) -> nn.Module\n",
      "wrap.__init__\n",
      "moe.__init__\n",
      "moe.top2gate\n",
      "moe.top2gate.gumbel_rsample(shape: Tuple, device: torch.device) -> Tensor\n",
      "moe.top2gate.one_hot(tensor: torch.Tensor, num_classes: int) -> Tensor\n",
      "moe.top2gate.top2gating(logits: torch.Tensor) -> Tuple[Tensor, Tensor, Tensor]\n",
      "moe.moe_layer\n",
      "internal.reduce_scatter_bucketer\n",
      "internal.params\n",
      "internal.params.get_global_rank(group: Any, rank: int) -> int\n",
      "internal.params.recursive_copy_to_device(value: Any, non_blocking: bool, device: torch.device) -> Any\n",
      "internal.params.calc_grad_norm(parameters: List[torch.nn.Parameter], p: float) -> torch.Tensor\n",
      "internal.state_dict\n",
      "internal.state_dict.find_module_instances(module: nn.Module, search_class: Type[nn.Module]) -> List[Tuple[str, nn.Module]]\n",
      "internal.state_dict.replace_by_prefix_(state_dict: Union[Dict[str, Tensor], 'OrderedDict[str, Tensor]'], old_prefix: str, new_prefix: str) -> None\n",
      "internal.version\n",
      "internal.version.torch_version(version: str) -> Tuple[int, ...]\n",
      "internal.object\n",
      "internal.object.pyobject_to_tensor(obj: Any, fixed_buffer_size: int) -> torch.Tensor\n",
      "internal.object.tensor_to_pyobject(tensor: torch.Tensor) -> Any\n",
      "internal.containers\n",
      "internal.containers.apply_to_type(type_fn: Callable, fn: Callable, container: Union[torch.Tensor, np.ndarray, Dict, List, Tuple, Set, NamedTuple]) -> Any\n",
      "internal.containers.apply_to_tensors(fn: Callable, container: Union[torch.Tensor, Dict, List, Tuple, Set]) -> Any\n",
      "internal.containers.to_np(tensor_or_container: Union[torch.Tensor, Dict, List, Tuple, Set]) -> Any\n",
      "internal.containers.from_np(ndarray_or_container: Union[np.ndarray, Dict, List, Tuple, Set]) -> Any\n",
      "internal.containers.pack_kwargs() -> Tuple[Tuple[str, ...], Tuple[Any, ...]]\n",
      "internal.containers.unpack_kwargs(kwarg_keys: Tuple[str, ...], flat_args: Tuple[Any, ...]) -> Tuple[Tuple[Any, ...], Dict[str, Any]]\n",
      "internal.containers.split_non_tensors(mixed: Union[torch.Tensor, Tuple[Any, ...]]) -> Tuple[Tuple[torch.Tensor, ...], Optional[Dict[str, List[Any]]]]\n",
      "internal.containers.unpack_non_tensors(tensors: Tuple[torch.Tensor, ...], packed_non_tensors: Optional[Dict[str, List[Any]]]) -> Tuple[Any, ...]\n",
      "internal.__init__\n",
      "internal.parallel\n",
      "internal.parallel.chunk_and_pad(tensor: torch.Tensor, num_chunks: int) -> List[torch.Tensor]\n",
      "internal.parallel.validate_process_group(device: torch.device, process_group: 'ProcessGroup') -> None\n",
      "internal.parallel.enable_pytorch_sync_bn(module: torch.nn.Module) -> None\n",
      "internal.parallel.get_process_group_cached(name: ProcessGroupName, ranks: Optional[Sequence[int]]) -> 'ProcessGroup'\n",
      "optim.grad_scaler\n",
      "optim.grad_scaler._refresh_per_optimizer_state() -> Dict\n",
      "optim.__init__\n",
      "optim.oss\n",
      "optim.oss._gpu_capabilities_older_than_50() -> bool\n",
      "optim.oss._broadcast_object(obj: Any, src_rank: int, group: object, dist_device: torch.device) -> Any\n",
      "optim.adam\n",
      "optim.layerwise_gradient_scaler\n",
      "optim.adascale\n"
     ]
    }
   ],
   "source": [
    "parsed_modules = []\n",
    "parser = PythonParser()\n",
    "for file in repo_path.glob('**/*.py'):\n",
    "    module = parser.parse_file(file)\n",
    "    parsed_modules.append(module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_function(func: FunctionElement, indent=0):\n",
    "    # print function content\n",
    "    print(' ' * indent + func.name)\n",
    "    for param in func.parameters:\n",
    "        print(' ' * indent + 'Parameter: ' + param)\n",
    "    #get the function body\n",
    "    # Use lineno and ModuleElement.body to get the body\n",
    "    module = func.module\n",
    "    body = module.body\n",
    "    print(' ' * indent + 'Body: ' + body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_parser.py:PythonParser(BaseParser):__init__(self: Any) -> Any\n",
      "Parameter: self: Any\n",
      "Body: \"\"\"\n",
      "Python-specific code parser implementation.\n",
      "\"\"\"\n",
      "\n",
      "import ast\n",
      "from pathlib import Path\n",
      "from typing import List, Optional, Dict, Any, Union\n",
      "from dataclasses import dataclass\n",
      "\n",
      "from .base_parser import BaseParser\n",
      "from ..models.data_models import (\n",
      "    ModuleElement, ClassElement, FunctionElement, DocumentationElement\n",
      ")\n",
      "\n",
      "@dataclass\n",
      "class ContextInfo:\n",
      "    \"\"\"Helper class to track parsing context.\"\"\"\n",
      "    module: Optional[ModuleElement] = None\n",
      "    parent_class: Optional[ClassElement] = None\n",
      "    parent_function: Optional[FunctionElement] = None\n",
      "    in_async_def: bool = False\n",
      "\n",
      "class PythonParser(BaseParser):\n",
      "    \"\"\"Parser for Python source code files.\"\"\"\n",
      "    \n",
      "    language = 'Python'\n",
      "\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "\n",
      "    def can_parse(self, path: Path) -> bool:\n",
      "        \"\"\"Check if file is a Python file.\"\"\"\n",
      "        return path.suffix.lower() in self.get_supported_extensions()\n",
      "\n",
      "    def get_supported_extensions(self) -> List[str]:\n",
      "        \"\"\"Get supported Python file extensions.\"\"\"\n",
      "        return ['.py', '.pyw']\n",
      "\n",
      "    def parse_file(self, path: Path) -> ModuleElement:\n",
      "        \"\"\"\n",
      "        Parse a Python source file.\n",
      "        \n",
      "        Args:\n",
      "            path: Path to the Python file\n",
      "            \n",
      "        Returns:\n",
      "            ModuleElement containing the parsed information\n",
      "        \"\"\"\n",
      "        try:\n",
      "            with open(path, 'r', encoding='utf-8') as f:\n",
      "                content = f.read()\n",
      "                \n",
      "            # Parse the AST\n",
      "            tree = ast.parse(content)\n",
      "            \n",
      "            # Create module element first\n",
      "            module = ModuleElement(\n",
      "                name=str(path),  # Full path as module name\n",
      "                path=path,\n",
      "                language=self.language,\n",
      "                classes=[],\n",
      "                functions=[],\n",
      "                imports=[],\n",
      "                documentation=None,\n",
      "                body=content\n",
      "            )\n",
      "            \n",
      "            # Create initial context\n",
      "            context = ContextInfo(module=module)\n",
      "            \n",
      "            # Extract module docstring\n",
      "            module.documentation = self._parse_docstring(tree)\n",
      "            \n",
      "            # Parse all module elements\n",
      "            for node in ast.iter_child_nodes(tree):\n",
      "                if isinstance(node, ast.ClassDef):\n",
      "                    module.classes.append(self._parse_class(path, node, context, str(context.module.name)))\n",
      "                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
      "                    module.functions.append(self._parse_function(path, node, context, str(context.module.name)))\n",
      "                elif isinstance(node, (ast.Import, ast.ImportFrom)):\n",
      "                    module.imports.extend(self._parse_imports(node))\n",
      "            return module\n",
      "            \n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error parsing {path}: {e}\")\n",
      "            return self._create_error_module(path, e)\n",
      "\n",
      "    def _parse_class(self, path: Path, node: ast.ClassDef, context: ContextInfo, parent_name: str) -> ClassElement:\n",
      "        \"\"\"Parse a class definition.\"\"\"\n",
      "        # Build qualified name based on context\n",
      "        qualified_name = [parent_name, f\"{node.name}({', '.join(self._get_name(base) for base in node.bases)})\"]\n",
      "        \n",
      "        # Create class element first\n",
      "        class_element = ClassElement(\n",
      "            name=\":\".join(qualified_name),  # <parent_name>:<parent_name>....<class_name>\n",
      "            path=path,\n",
      "            documentation=None,\n",
      "            methods=[],\n",
      "            base_classes=[],\n",
      "            attributes={},\n",
      "            decorators=[],\n",
      "            start_line=node.lineno,\n",
      "            end_line=node.end_lineno,\n",
      "            module=context.module\n",
      "        )\n",
      "        \n",
      "        # Create new context for class contents\n",
      "        class_context = ContextInfo(\n",
      "            module=context.module,\n",
      "            parent_class=class_element,\n",
      "            parent_function=context.parent_function,\n",
      "            in_async_def=context.in_async_def\n",
      "        )\n",
      "        \n",
      "        # Get docstring and decorators\n",
      "        class_element.documentation = self._parse_docstring(node)\n",
      "        class_element.decorators = self._parse_decorators(node)\n",
      "        \n",
      "        # Parse methods and nested classes\n",
      "        for body_node in node.body:\n",
      "            if isinstance(body_node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
      "                class_element.methods.append(self._parse_function(path, body_node, class_context, class_element.name))\n",
      "            elif isinstance(body_node, ast.ClassDef):\n",
      "                class_element.inner_classes.append(self._parse_class(path, body_node, class_context, class_element.name))\n",
      "            elif isinstance(body_node, ast.Assign):\n",
      "                # Class attributes\n",
      "                for target in body_node.targets:\n",
      "                    if isinstance(target, ast.Name):\n",
      "                        class_element.attributes[target.id] = self._get_attribute_type(body_node.value)\n",
      "                        \n",
      "        class_element.base_classes = [self._get_name(base) for base in node.bases]\n",
      "        \n",
      "        return class_element\n",
      "\n",
      "    def _parse_decorators(self, node: Union[ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef]) -> List[str]:\n",
      "        \"\"\"\n",
      "        Parse decorators from a class or function node.\n",
      "        \n",
      "        Args:\n",
      "            node: AST node for class or function\n",
      "            \n",
      "        Returns:\n",
      "            List of decorator strings\n",
      "        \"\"\"\n",
      "        decorators = []\n",
      "        for decorator in node.decorator_list:\n",
      "            try:\n",
      "                if isinstance(decorator, ast.Call):\n",
      "                    # Handle decorators with arguments: @decorator(arg1, arg2)\n",
      "                    decorator_name = self._get_decorator_name(decorator.func)\n",
      "                    args = []\n",
      "                    \n",
      "                    # Process positional arguments\n",
      "                    for arg in decorator.args:\n",
      "                        if isinstance(arg, ast.Constant):\n",
      "                            args.append(repr(arg.value))\n",
      "                        elif isinstance(arg, ast.Name):\n",
      "                            args.append(arg.id)\n",
      "                        else:\n",
      "                            args.append(ast.unparse(arg))\n",
      "                    \n",
      "                    # Process keyword arguments\n",
      "                    for keyword in decorator.keywords:\n",
      "                        if isinstance(keyword.value, ast.Constant):\n",
      "                            args.append(f\"{keyword.arg}={repr(keyword.value.value)}\")\n",
      "                        else:\n",
      "                            args.append(f\"{keyword.arg}={ast.unparse(keyword.value)}\")\n",
      "                    \n",
      "                    decorators.append(f\"@{decorator_name}({', '.join(args)})\")\n",
      "                \n",
      "                elif isinstance(decorator, ast.Attribute):\n",
      "                    # Handle decorators with attributes: @module.decorator\n",
      "                    decorators.append(f\"@{self._get_decorator_name(decorator)}\")\n",
      "                \n",
      "                elif isinstance(decorator, ast.Name):\n",
      "                    # Handle simple decorators: @decorator\n",
      "                    decorators.append(f\"@{decorator.id}\")\n",
      "                \n",
      "                else:\n",
      "                    # Handle any other decorator forms\n",
      "                    decorators.append(f\"@{ast.unparse(decorator)}\")\n",
      "                    \n",
      "            except Exception as e:\n",
      "                self.logger.warning(f\"Error parsing decorator: {e}\")\n",
      "                decorators.append(f\"@<error_parsing_decorator>\")\n",
      "        \n",
      "        return decorators\n",
      "\n",
      "    def _get_decorator_name(self, node: Union[ast.Name, ast.Attribute]) -> str:\n",
      "        \"\"\"\n",
      "        Get the full name of a decorator node.\n",
      "        \n",
      "        Args:\n",
      "            node: AST node representing the decorator name\n",
      "            \n",
      "        Returns:\n",
      "            Full decorator name as a string\n",
      "        \"\"\"\n",
      "        if isinstance(node, ast.Name):\n",
      "            return node.id\n",
      "        elif isinstance(node, ast.Attribute):\n",
      "            return f\"{self._get_decorator_name(node.value)}.{node.attr}\"\n",
      "        return ast.unparse(node)\n",
      "\n",
      "    def _parse_function(self, path: Path, node: Union[ast.FunctionDef, ast.AsyncFunctionDef], context: ContextInfo, parent_name: str) -> FunctionElement:\n",
      "        \"\"\"Parse a function or method definition.\"\"\"\n",
      "        # Get parameters and return type\n",
      "        params = []\n",
      "        for arg in node.args.args:\n",
      "            param_type = self._get_annotation_type(arg.annotation)\n",
      "            params.append(f\"{arg.arg}: {param_type}\")\n",
      "        return_type = self._get_annotation_type(node.returns)\n",
      "        \n",
      "        # Build qualified name based on context\n",
      "        qualified_name = [parent_name, f\"{node.name}({', '.join(params)}) -> {return_type}\"]\n",
      "        \n",
      "        # Create function element\n",
      "        function_element = FunctionElement(\n",
      "            name=\":\".join(qualified_name),  # <parent_name>:<parent_name>....<function_name>\n",
      "            path=path,\n",
      "            module=context.module,\n",
      "            documentation=None,\n",
      "            parameters=params,\n",
      "            return_type=return_type,\n",
      "            decorators=[],\n",
      "            complexity=None,\n",
      "            start_line=node.lineno,\n",
      "            end_line=node.end_lineno,\n",
      "            is_async=isinstance(node, ast.AsyncFunctionDef)\n",
      "        )\n",
      "        \n",
      "        # Create new context for function contents\n",
      "        function_context = ContextInfo(\n",
      "            module=context.module,\n",
      "            parent_class=context.parent_class,\n",
      "            parent_function=function_element,\n",
      "            in_async_def=isinstance(node, ast.AsyncFunctionDef)\n",
      "        )\n",
      "        \n",
      "        # Get docstring and decorators\n",
      "        function_element.documentation = self._parse_docstring(node)\n",
      "        function_element.decorators = self._parse_decorators(node)\n",
      "        \n",
      "        # Calculate complexity\n",
      "        function_element.complexity = self._calculate_complexity(node)\n",
      "        \n",
      "        return function_element\n",
      "\n",
      "    def _parse_imports(self, node: Union[ast.Import, ast.ImportFrom]) -> List[str]:\n",
      "        \"\"\"Parse import statements.\"\"\"\n",
      "        imports = []\n",
      "        if isinstance(node, ast.Import):\n",
      "            for name in node.names:\n",
      "                imports.append(name.name)\n",
      "        else:  # ImportFrom\n",
      "            module = node.module or ''\n",
      "            for name in node.names:\n",
      "                imports.append(f\"from {module} import {name.name}\")\n",
      "        return imports\n",
      "\n",
      "    def _parse_docstring(self, node: ast.AST) -> Optional[DocumentationElement]:\n",
      "        \"\"\"Extract docstring from an AST node.\"\"\"\n",
      "        docstring = ast.get_docstring(node)\n",
      "        if docstring:\n",
      "            return DocumentationElement(\n",
      "                content=docstring,\n",
      "                path=str(getattr(node, 'lineno', 0)),\n",
      "                line_number=getattr(node, 'lineno', 0),\n",
      "                type='docstring'\n",
      "            )\n",
      "        return None\n",
      "\n",
      "    def _calculate_complexity(self, node: ast.AST) -> int:\n",
      "        \"\"\"\n",
      "        Calculate cyclomatic complexity of a function.\n",
      "        Very basic implementation - counts branches.\n",
      "        \"\"\"\n",
      "        complexity = 1  # Base complexity\n",
      "        \n",
      "        for child in ast.walk(node):\n",
      "            # Count branches\n",
      "            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor,\n",
      "                                ast.ExceptHandler, ast.With, ast.AsyncWith)):\n",
      "                complexity += 1\n",
      "            # Count boolean operations\n",
      "            elif isinstance(child, ast.BoolOp):\n",
      "                complexity += len(child.values) - 1\n",
      "                \n",
      "        return complexity\n",
      "\n",
      "    def _get_annotation_type(self, node: Optional[ast.AST]) -> str:\n",
      "        \"\"\"Convert annotation AST node to string representation.\"\"\"\n",
      "        if node is None:\n",
      "            return 'Any'\n",
      "        return ast.unparse(node)\n",
      "\n",
      "    def _get_attribute_type(self, node: ast.AST) -> str:\n",
      "        \"\"\"Get the type of a class attribute from its value.\"\"\"\n",
      "        if isinstance(node, ast.Constant):\n",
      "            return type(node.value).__name__\n",
      "        elif isinstance(node, ast.List):\n",
      "            return 'List'\n",
      "        elif isinstance(node, ast.Dict):\n",
      "            return 'Dict'\n",
      "        elif isinstance(node, ast.Set):\n",
      "            return 'Set'\n",
      "        return 'Any'\n",
      "\n",
      "    def _get_name(self, node: ast.AST) -> str:\n",
      "        \"\"\"Convert AST name node to string.\"\"\"\n",
      "        if isinstance(node, ast.Name):\n",
      "            return node.id\n",
      "        elif isinstance(node, ast.Attribute):\n",
      "            return f\"{self._get_name(node.value)}.{node.attr}\"\n",
      "        return ast.unparse(node)\n"
     ]
    }
   ],
   "source": [
    "traverse_function(module.classes[1].methods[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_parser.py:ContextInfo\n",
      "python_parser.py:PythonParser\n",
      "python_parser.py:__init__(self: Any) -> Any : 27\n",
      "python_parser.py:can_parse(self: Any, path: Path) -> bool : 31\n",
      "python_parser.py:get_supported_extensions(self: Any) -> List[str] : 35\n",
      "python_parser.py:parse_file(self: Any, path: Path) -> ModuleElement : 39\n",
      "python_parser.py:_parse_class(self: Any, path: Path, node: ast.ClassDef) -> ClassElement : 86\n",
      "python_parser.py:_parse_decorators(self: Any, node: Union[ast.ClassDef, ast.FunctionDef, ast.AsyncFunctionDef]) -> List[str] : 126\n",
      "python_parser.py:_get_decorator_name(self: Any, node: Union[ast.Name, ast.Attribute]) -> str : 180\n",
      "python_parser.py:_parse_function(self: Any, path: Path, node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> FunctionElement : 196\n",
      "python_parser.py:_parse_imports(self: Any, node: Union[ast.Import, ast.ImportFrom]) -> List[str] : 235\n",
      "python_parser.py:_parse_docstring(self: Any, node: ast.AST) -> Optional[DocumentationElement] : 247\n",
      "python_parser.py:_calculate_complexity(self: Any, node: ast.AST) -> int : 259\n",
      "python_parser.py:_get_annotation_type(self: Any, node: Optional[ast.AST]) -> str : 277\n",
      "python_parser.py:_get_attribute_type(self: Any, node: ast.AST) -> str : 283\n",
      "python_parser.py:_get_name(self: Any, node: ast.AST) -> str : 295\n"
     ]
    }
   ],
   "source": [
    "print_module(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
